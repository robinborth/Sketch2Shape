# @package _global_

defaults:
  - data:
    - optimize_latent 
  - model: optimize_deepsdf
  - trainer: default
  - logger: default
  - hydra: default
  - paths: default
  - debug: null
  - optional local: default
  - _self_

seed: 123
task_name: optimize_deepsdf
tags: ["optimize_deepsdf"]
split: val  # train, val, test
obj_ids: ???  # overrids the split setting, make sure that obj_ids are only from the same split.
train: True
prior_idx: mean  # random(-2), mean(-1), train(idx)
eval: True
save_mesh: True  # only if eval == True
create_video: True 
ckpt_path: ???

trainer:
  num_sanity_val_steps: 0
  max_epochs: 1000 

data:
  train_dataset:
    _target_: lib.data.deepsdf_dataset.DeepSDFLatentOptimizerDataset
    chunk_size: 250000 
    half: True

model:
  scheduler:
    _partial_: True
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 50 
    gamma: 0.9
  
callbacks:
  early_stopping:
    patience: 200