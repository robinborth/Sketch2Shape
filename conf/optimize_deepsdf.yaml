# @package _global_

defaults:
  - data: optimize_latent
  - dataset: shapenet_chair_4096 
  - model: optimize_latent
  - trainer: default
  - logger: default
  - hydra: default
  - paths: default
  - debug: null
  - optional local: default
  - _self_

seed: 123
task_name: optimize_deepsdf
tags: ["optimize_deepsdf"]
split: val  # train, train_latent, val, val_latent, test
obj_ids: ???  # overrids the split setting, make sure that obj_ids are only from the same split.
obj_dir: ???
train: True
eval: True
save_mesh: True
save_latent: True
create_video: True 
deepsdf_ckpt_path: ${paths.checkpoint_dir}/deepsdf.ckpt
loss_ckpt_path: ${paths.checkpoint_dir}/loss.ckpt

trainer:
  num_sanity_val_steps: 0
  max_epochs: 2000 

data:
  dataset:
    _target_: lib.data.dataset.optimize_latent.DeepSDFLatentOptimizerDataset
    chunk_size: 16384 
    half: True

model:
  # custom settings
  _target_: lib.optimizer.deepsdf.DeepSDFLatentOptimizer
  adaptive_sample_strategy: True
  adaptive_mining_strategy: True
  reg_loss: prior 
  reg_weight: 1e-04
  # base settings
  optimizer:
    lr: 1e-03
  scheduler:
    _partial_: True
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 500
    gamma: 0.5