# @package _global_

defaults:
  - data: optimize_latent
  - dataset: shapenet_chair_4096 
  - model: optimize_latent
  - trainer: default
  - logger: wandb
  - hydra: default
  - paths: default
  - debug: null
  - optional local: default
  - _self_

seed: 123
task_name: optimize_normals
tags: ["optimize_normals"]
split: val  # train, train_latent, val, val_latent, test
obj_ids: ???  # overrids the split setting, make sure that obj_ids are only from the same split.
train: True
eval: True
save_mesh: True
save_latent: True
create_video: True 
deepsdf_ckpt_path: ${paths.checkpoint_dir}/deepsdf.ckpt
loss_ckpt_path: ${paths.checkpoint_dir}/latent_traverse.ckpt

trainer:
  num_sanity_val_steps: 0
  reload_dataloaders_every_epoch: True
  max_epochs: 20 
  accumulate_grad_batches: 4

data:
  size: 256
  milestones: []
  dataset:
    _target_: lib.data.dataset.optimize_latent.NormalLatentOptimizerDataset
    azims: ${data.preprocess_eval_synthetic.azims}
    elevs: ${data.preprocess_eval_synthetic.elevs}
  shuffle: True

model:
  # custom settings
  _target_: lib.optimizer.normals.NormalsOptimizer
  reg_loss: prior
  reg_weight: 1e-01
  # base settings
  optimizer:
    lr: 5e-03
  scheduler:
    step_size: 5
    gamma: 0.5
  capture_rate: 8
