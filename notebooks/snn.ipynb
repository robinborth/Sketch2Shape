{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.config import load_config\n",
    "from lib.data.sampler import ChunkSampler\n",
    "from lib.data.metainfo import MetaInfo\n",
    "\n",
    "cfg = load_config(\n",
    "    \"train_loss\", [\"+experiment/train_loss=latent_encoder_shapenet_chair_4096\"]\n",
    ")\n",
    "metainfo = MetaInfo(cfg.data.data_dir, split=\"val_latent\")\n",
    "metainfo.load_loss()\n",
    "sampler = ChunkSampler(metainfo.loss_labels, chunk_size=1)\n",
    "max([i for i in sampler]), len([i for i in sampler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.data.metainfo import MetaInfo\n",
    "from lib.data.transforms import BaseTransform\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from lib.visualize.image import image_grid\n",
    "from torch.nn.functional import l1_loss\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "def transform(normal):\n",
    "    _transform = BaseTransform()\n",
    "    return _transform(normal).to(\"cuda\")\n",
    "\n",
    "\n",
    "def plot_images(images, size: int = 4):\n",
    "    if isinstance(images, list):\n",
    "        _, axes = plt.subplots(1, len(images), figsize=(size, size))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(size, size))\n",
    "        plt.imshow(images)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def siamese_loss(emb_1, emb_2):\n",
    "    return 1 - cosine_similarity(emb_1, emb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'lib.optimizer.sketch.SketchOptimizer':\nAttributeError(\"module 'lib.data.transforms' has no attribute 'ToGrayScale'\")\nfull_key: model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_target_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/sketch2shape/lib/optimizer/sketch.py:14\u001b[0m, in \u001b[0;36mSketchOptimizer.__init__\u001b[0;34m(self, loss_weight, silhouette_loss, silhouette_weight, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      9\u001b[0m     loss_weight: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     13\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sketch2shape/lib/optimizer/latent.py:73\u001b[0m, in \u001b[0;36mLatentOptimizer.__init__\u001b[0;34m(self, data_dir, loss_ckpt_path, deepsdf_ckpt_path, optimizer, scheduler, latent_init, sketch_mode, retrieval_mode, reg_loss, reg_weight, prior_obj_id, prior_view_id, retrieval_k, mesh_resolution, mesh_chunk_size, n_render_steps, clamp_sdf, step_scale, surface_eps, sphere_eps, normal_eps, log_images, capture_rate, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepsdf\u001b[38;5;241m.\u001b[39mcreate_camera()\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m \u001b[43mLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_ckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03mpassed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/fabric/utilities/cloud_io.py:56\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/pickle.py:1529\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1528\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1529\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(klass)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/utilities/migration/utils.py:198\u001b[0m, in \u001b[0;36m_RedirectingUnpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    197\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRedirecting import of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/pickle.py:1584\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lib.data.transforms' has no attribute 'ToGrayScale'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlatent_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mretrieval_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m sketch_3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(metainfo\u001b[38;5;241m.\u001b[39mload_image(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     11\u001b[0m sketch_5 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(metainfo\u001b[38;5;241m.\u001b[39mload_image(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     _convert_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mCONVERT, ConvertMode\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[1;32m    224\u001b[0m     _partial_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mPARTIAL, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_recursive_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_partial_\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m OmegaConf\u001b[38;5;241m.\u001b[39mis_list(config):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     config_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[0m, in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 value \u001b[38;5;241m=\u001b[39m instantiate_node(\n\u001b[1;32m    343\u001b[0m                     value, convert\u001b[38;5;241m=\u001b[39mconvert, recursive\u001b[38;5;241m=\u001b[39mrecursive\n\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             kwargs[key] \u001b[38;5;241m=\u001b[39m _convert_node(value, convert)\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;241m==\u001b[39m ConvertMode\u001b[38;5;241m.\u001b[39mALL \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m         convert \u001b[38;5;129;01min\u001b[39;00m (ConvertMode\u001b[38;5;241m.\u001b[39mPARTIAL, ConvertMode\u001b[38;5;241m.\u001b[39mOBJECT)\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mobject_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    354\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_key:\n\u001b[1;32m     96\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfull_key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Error in call to target 'lib.optimizer.sketch.SketchOptimizer':\nAttributeError(\"module 'lib.data.transforms' has no attribute 'ToGrayScale'\")\nfull_key: model"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"optimize_sketch\", [\"dataset=shapenet_chair_single_view_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[4015]\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\"\n",
    "cfg.model.latent_init = \"retrieval\"\n",
    "cfg.model.retrieval_k = 4\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "\n",
    "sketch_3 = np.asarray(metainfo.load_image(3, 11, 0))\n",
    "sketch_5 = np.asarray(metainfo.load_image(5, 11, 0))\n",
    "rendered_normal_3 = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "rendered_normal_3_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "sketch_3_emb = model.loss(transform(sketch_3)[None, ...])\n",
    "sketch_5_emb = model.loss(transform(sketch_5)[None, ...])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_3_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_3, rendered_normal_3])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_5_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_5, rendered_normal_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.retrieval_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis of Possible Rendering Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_id = 4009\n",
    "\n",
    "gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "model.latent = sketch_emb\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "model.latent = gt_emb\n",
    "gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "# loss_gt_sketch = l1_loss(gt_emb, sketch_emb).item()\n",
    "loss_sketch_normal = l1_loss(sketch_emb, rendered_normal_emb).item()\n",
    "loss_sketch_gt_normal = l1_loss(sketch_emb, gt_rendered_normal_emb).item()\n",
    "# print(f\"loss_gt_sketch={loss_gt_sketch:.4f}\")\n",
    "print(f\"loss_sketch_normal={loss_sketch_normal:.4f}\")\n",
    "print(f\"loss_sketch_gt_normal={loss_sketch_gt_normal:.4f}\")\n",
    "plot_images([sketch, rendered_normal, gt_rendered_normal], size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Analysis Of Possible Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "loss_gt_sketch = []\n",
    "loss_sketch_normal = []\n",
    "loss_sketch_gt_normal = []\n",
    "for shape_id in tqdm(range(3968, 3968 + 128, 1)):  # validation split\n",
    "    # for shape_id in tqdm(range(256)):  # train split\n",
    "    gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "    sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "    sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "    model.latent = sketch_emb\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "    model.latent = gt_emb\n",
    "    gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "    loss_gt_sketch.append(l1_loss(gt_emb, sketch_emb).item())\n",
    "    loss_sketch_normal.append(l1_loss(sketch_emb, rendered_normal_emb).item())\n",
    "    loss_sketch_gt_normal.append(l1_loss(sketch_emb, gt_rendered_normal_emb).item())\n",
    "\n",
    "mean_loss_gt_sketch = np.array(loss_gt_sketch).mean()\n",
    "mean_loss_sketch_normal = np.array(loss_sketch_normal).mean()\n",
    "mean_loss_sketch_gt_normal = np.array(loss_sketch_gt_normal).mean()\n",
    "\n",
    "print(f\"mean_loss_gt_sketch={mean_loss_gt_sketch:.4f}\")\n",
    "print(f\"mean_loss_sketch_normal={mean_loss_sketch_normal:.4f}\")\n",
    "print(f\"mean_loss_sketch_gt_normal={mean_loss_sketch_gt_normal:.4f}\")\n",
    "print(f\"mean_gain={(mean_loss_sketch_normal-mean_loss_sketch_gt_normal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Closest Latent Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.norm(\n",
    "    model.deepsdf.lat_vecs.weight[0] - model.deepsdf.lat_vecs.weight, dim=-1\n",
    ")\n",
    "torch.argsort(idxs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = model.shape_latents.std(0)\n",
    "mean = model.shape_latents.mean(0)\n",
    "loss = []\n",
    "images = []\n",
    "for idx in model.shape_idx:\n",
    "    reg_loss = torch.nn.functional.l1_loss(\n",
    "        model.deepsdf.lat_vecs.weight[idx], model.deepsdf.lat_vecs.weight[2]\n",
    "    )\n",
    "    model.latent = model.deepsdf.lat_vecs.weight[idx]\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(rendered_normal)\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "    loss.append(reg_loss.mean().item())\n",
    "    print(idx, reg_loss.mean())\n",
    "mean_loss = np.array(loss).mean()\n",
    "print(f\"mean_loss={mean_loss:.04}\")\n",
    "for l in loss:\n",
    "    print(f\"{l:.03}\")\n",
    "plot_images(images, size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[3]\n",
    "model.deepsdf.create_camera(azim=40, elev=-30)\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "mean = rendered_normal.mean(-1)\n",
    "mean = np.stack([mean, mean, mean], axis=-1)\n",
    "rendered_gray_emb = model.loss(transform(mean)[None, ...])\n",
    "loss = siamese_loss(rendered_gray_emb, sketch_3_emb)\n",
    "print(loss)\n",
    "plot_images(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative/Qualitative Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "# start_latent_ids = [3, 3385, 2801, 1962, 1058, 782, 1328]  # couch nn\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [0, 5]  # couch far\n",
    "end_latent_id = 0\n",
    "sketch_view = 11\n",
    "traversal_steps = 20\n",
    "image_skip = 2\n",
    "azims = [40]\n",
    "elevs = [-30]\n",
    "\n",
    "# fetch and encode the sketch\n",
    "sketch = metainfo.load_image(end_latent_id, sketch_view, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectories = []\n",
    "loss_trajectories = []\n",
    "for idx, start_latent_id in enumerate(start_latent_ids):\n",
    "    image_trajectory = [sketch]\n",
    "    loss_trajectory = []\n",
    "    desc = f\"{idx+1}/{len(start_latent_ids)}\"\n",
    "    for t in tqdm(np.linspace(1, 0, traversal_steps), desc=desc):\n",
    "        start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "        end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "\n",
    "        # calculate the mean loss from all the views\n",
    "        loss = []\n",
    "        for azim in azims:\n",
    "            for elev in elevs:\n",
    "                model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "                rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "                # mean = rendered_normal.mean(-1)\n",
    "                # rendered_normal = np.stack([mean, mean,mean], axis=-1)\n",
    "                rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])\n",
    "                snn_loss = siamese_loss(sketch_emb, rendered_normal_emb)\n",
    "                loss.append(snn_loss)\n",
    "                image_trajectory.append(rendered_normal)\n",
    "        loss = torch.stack(loss).mean()\n",
    "\n",
    "        # add the loss to the trajectory\n",
    "        loss_trajectory.append(loss.detach().cpu().numpy())\n",
    "    loss_trajectories.append(loss_trajectory)\n",
    "    image_trajectories.append(image_trajectory)\n",
    "\n",
    "# plot the images\n",
    "for image_trajectory in image_trajectories:\n",
    "    trajectory = []\n",
    "    for idx, img in enumerate(image_trajectory):\n",
    "        if idx == 0 or (idx - 1) % image_skip == 0:\n",
    "            trajectory.append(img)\n",
    "    plot_images(trajectory, size=16)\n",
    "\n",
    "# plot the loss curves\n",
    "for obj_id, loss_trajectory in zip(start_latent_ids, loss_trajectories):\n",
    "    plt.plot(\n",
    "        np.linspace(0, 1, traversal_steps),\n",
    "        np.stack(loss_trajectory),\n",
    "        label=obj_id,\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "mean = np.stack(loss_trajectories).mean(0)\n",
    "std = np.stack(loss_trajectories).std(0)\n",
    "plt.plot(x, mean)\n",
    "plt.fill_between(x, (mean - std), (mean + std), color=\"b\", alpha=0.1)\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[0]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "normal_emb = model.loss(transform(normal)[None, ...])\n",
    "print(f\"{siamese_loss(sketch_3_emb, normal_emb)=}\")\n",
    "plot_images([sketch_3, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensor(2549, device='cuda:0') tensor(0.6049, device='cuda:0')\n",
    "tensor(1328, device='cuda:0') tensor(1.4948, device='cuda:0')\n",
    "tensor(1898, device='cuda:0') tensor(0.6908, device='cuda:0')\n",
    "tensor(962, device='cuda:0') tensor(0.6423, device='cuda:0')\n",
    "tensor(535, device='cuda:0') tensor(0.8771, device='cuda:0')\n",
    "tensor(755, device='cuda:0') tensor(0.4972, device='cuda:0')\n",
    "tensor(2432, device='cuda:0') tensor(1.8280, device='cuda:0')\n",
    "tensor(3089, device='cuda:0') tensor(0.9243, device='cuda:0')\n",
    "tensor(3885, device='cuda:0') tensor(1.6862, device='cuda:0')\n",
    "tensor(1447, device='cuda:0') tensor(1.0207, device='cuda:0')\n",
    "tensor(3195, device='cuda:0') tensor(0.8885, device='cuda:0')\n",
    "tensor(2207, device='cuda:0') tensor(0.7391, device='cuda:0')\n",
    "tensor(277, device='cuda:0') tensor(0.7135, device='cuda:0')\n",
    "tensor(1058, device='cuda:0') tensor(0.6626, device='cuda:0')\n",
    "tensor(841, device='cuda:0') tensor(1.0481, device='cuda:0')\n",
    "tensor(1637, device='cuda:0') tensor(0.6819, device='cuda:0')\n",
    "\"\"\"\n",
    "\n",
    "model.latent = model.deepsdf.lat_vecs.weight[3885]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "\n",
    "mean = model.shape_latents.mean(0)\n",
    "std = model.shape_latents.std(0)\n",
    "\n",
    "good_latents = []\n",
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0 and i != 3:\n",
    "        good_latents.append(latent)\n",
    "\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"init\", i, reg_loss)\n",
    "\n",
    "mean = torch.stack(good_latents).mean(0)\n",
    "std = torch.stack(good_latents).std(0)\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"good\", i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0:\n",
    "        print(i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[1962]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "# mean = model.shape_latents.mean(0)\n",
    "# std = model.shape_latents.std(0)\n",
    "# mean = torch.stack(good_latents).mean(0)\n",
    "# std = torch.stack(good_latents).std(0)\n",
    "\n",
    "((mean - latent) / std).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Latent Code (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_id = 3385\n",
    "end_latent_id = 3\n",
    "optim_steps = 10\n",
    "\n",
    "t = torch.tensor(1.0, dtype=torch.float32).to(\"cuda\")\n",
    "t.requires_grad = True\n",
    "start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "start_latent.requires_grad = True\n",
    "end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "end_latent.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([t], lr=0.1)\n",
    "\n",
    "sketch = metainfo.load_image(end_latent_id, 11, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectory = [np.asarray(sketch)]\n",
    "loss_trajectory = []\n",
    "reg_loss_trajectory = []\n",
    "t_trajectory = []\n",
    "with tqdm(total=optim_steps) as pbar:\n",
    "    for step in range(optim_steps):\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "        points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "            latent=model.latent,\n",
    "            points=model.deepsdf.camera_points,\n",
    "            rays=model.deepsdf.camera_rays,\n",
    "            mask=model.deepsdf.camera_mask,\n",
    "        )\n",
    "        rendered_normal = model.deepsdf.render_normals(\n",
    "            latent=model.latent,\n",
    "            points=points,\n",
    "            mask=surface_mask,\n",
    "        )  # (H, W, 3)\n",
    "        normal = model.deepsdf.image_to_siamese(rendered_normal)  # (1, 3, H, W)\n",
    "        normal_emb = model.loss(normal)  # (1, D)\n",
    "\n",
    "        snn_loss = siamese_loss(sketch_emb, normal_emb)\n",
    "\n",
    "        std = model.shape_latents.std(0)\n",
    "        mean = model.shape_latents.mean(0)\n",
    "        reg_loss = ((model.latent.clone() - mean) / std).pow(2)\n",
    "        reg_loss_trajectory.append(reg_loss.mean().item())\n",
    "\n",
    "        loss_trajectory.append(snn_loss.item())\n",
    "        t_trajectory.append(t.item())\n",
    "        image_trajectory.append(rendered_normal.detach().cpu().numpy())\n",
    "\n",
    "        snn_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_postfix({\"t\": t.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "# images\n",
    "plot_images(image_trajectory, size=16)\n",
    "\n",
    "# loss\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(reg_loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"reg_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(t_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random from Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_latent_ids = [0, 3176, 2110, 2733, 2317, 94, 3831]  # chair nn\n",
    "mean = model.deepsdf.lat_vecs.weight[5]\n",
    "std = model.deepsdf.lat_vecs.weight.std(0)\n",
    "latent = mean + torch.randn_like(std) * std * 0.3\n",
    "print(l1_loss(latent, mean))\n",
    "model.latent = latent\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "start_latent_id = 1\n",
    "mean = model.deepsdf.lat_vecs.weight[0]\n",
    "images = []\n",
    "for t in np.linspace(0, 1, 11):\n",
    "    latent = t * model.deepsdf.lat_vecs.weight[start_latent_id] + (1 - t) * mean\n",
    "    model.latent = latent\n",
    "    normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(normal)\n",
    "plot_images(images, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sketch(img):\n",
    "    img = cv.Canny(\n",
    "        (img * 255).astype(np.uint8),\n",
    "        100,\n",
    "        150,\n",
    "        L2gradient=True,\n",
    "        apertureSize=3,\n",
    "    )\n",
    "    img = cv.bitwise_not(img)\n",
    "    return np.stack([img, img, img], axis=-1).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "# for i in torch.randint(4096, (100,)):\n",
    "for i in [0] * 20:\n",
    "    t = torch.normal(torch.tensor(0.25), torch.tensor(0.1))\n",
    "    t = torch.clamp(t, 0.0, 0.5)\n",
    "    # azim = torch.normal(torch.tensor(30.0), torch.tensor(5))\n",
    "    # azim = torch.normal(torch.tensor(45.0), torch.tensor(22.5))\n",
    "    azim = torch.normal(torch.tensor(90.0), torch.tensor(22.5))\n",
    "    elev = torch.normal(torch.tensor(-20.0), torch.tensor(5))\n",
    "    model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "    print(f\"{t=}\")\n",
    "    print(f\"{azim=}\")\n",
    "    print(f\"{elev=}\")\n",
    "    source_id = 0\n",
    "    target_id = i\n",
    "\n",
    "    source_latent = model.deepsdf.lat_vecs.weight[source_id]\n",
    "    model.latent = source_latent\n",
    "    source_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    target_latent = model.deepsdf.lat_vecs.weight[target_id]\n",
    "    model.latent = target_latent\n",
    "    target_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    interpolated_latent = (1 - t) * source_latent + t * target_latent\n",
    "    model.latent = interpolated_latent\n",
    "    interpolated_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    print(l1_loss(source_latent, target_latent))\n",
    "    plot_images(\n",
    "        [\n",
    "            source_normal,\n",
    "            target_normal,\n",
    "            interpolated_normal,\n",
    "            to_sketch(interpolated_normal),\n",
    "        ],\n",
    "        size=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.preprocess import PreprocessRenderings\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "preprocess = PreprocessRenderings(\n",
    "    data_dir=\"/home/borth/sketch2shape/data/shapenet_chair_4096\",\n",
    "    deepsdf_ckpt_path=\"/home/borth/sketch2shape/checkpoints/deepsdf.ckpt\",\n",
    "    n_renderings=10,\n",
    ")\n",
    "normals, sketches, latents, config = preprocess.preprocess(obj_id=metainfo.obj_ids[17])\n",
    "plot_images(normals, size=16)\n",
    "plot_images(sketches, size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandDrawn Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lib.data.transforms import BaseTransform, ToGrayScale, DilateSketch, ToSketch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[0]\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_multi_view_256.ckpt\"\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\"\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_64.ckpt\"\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_128.ckpt\"\n",
    "\n",
    "cfg.model.latent_init = \"latent\"\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "rendered_normal_latent = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "# path = \"/home/borth/sketch2shape/temp/chair1.png\"\n",
    "# img = Image.open(path).convert(\"RGB\")\n",
    "# path = \"/home/borth/sketch2shape/temp/chair2.png\"\n",
    "# img = Image.open(path).convert(\"RGB\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4117], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4112], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4126], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4130], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4105], \"00011\")\n",
    "img = metainfo.load_sketch(metainfo.obj_ids[4107], \"00011\")\n",
    "img = metainfo.load_sketch(metainfo.obj_ids[4108], \"00011\")\n",
    "\n",
    "transforms1 = [v2.Resize((256, 256))]\n",
    "transforms2 = [v2.Resize((256, 256)), ToSketch()]\n",
    "transforms3 = [v2.Resize((256, 256)), DilateSketch(kernel_size=5)]\n",
    "transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "\n",
    "# transforms1 = [v2.Resize((256, 256)), v2.Resize((64, 64), antialias=True)]\n",
    "# transforms2 = [v2.Resize((256, 256)), ToSketch(),v2.Resize((64, 64), antialias=True)]\n",
    "# transforms3 = [v2.Resize((256, 256), antialias=True), DilateSketch(kernel_size=5),v2.Resize((64, 64), antialias=True)]\n",
    "# transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5),v2.Resize((64, 64), antialias=True)]\n",
    "\n",
    "# transforms1 = [v2.Resize((256, 256)), v2.Resize((128, 128), antialias=True)]\n",
    "# transforms2 = [v2.Resize((256, 256)), ToSketch(),v2.Resize((128, 128), antialias=True)]\n",
    "# transforms3 = [v2.Resize((256, 256), antialias=True), DilateSketch(kernel_size=5),v2.Resize((128, 128), antialias=True)]\n",
    "# transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5),v2.Resize((128, 128), antialias=True)]\n",
    "\n",
    "\n",
    "def create_sketch(image, transforms):\n",
    "    trans = BaseTransform(transforms=transforms)\n",
    "    to_image = BaseTransform(normalize=False, transforms=transforms)\n",
    "    sketch = trans(image)[None, ...].to(\"cuda\")\n",
    "    model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "    # model.deepsdf.hparams[\"surface_eps\"] = 5e-02\n",
    "    model.deepsdf.hparams[\"surface_eps\"] = 1e-03\n",
    "    # model.deepsdf.create_camera(azim=60, elev=-25)\n",
    "    # model.deepsdf.create_camera(azim=120, elev=-10)\n",
    "    model.deepsdf.create_camera(azim=40, elev=-20)\n",
    "    rendered_normal = model.capture_camera_frame(\"grayscale\").detach().cpu().numpy()\n",
    "    return to_image(image).permute(1, 2, 0), rendered_normal\n",
    "\n",
    "\n",
    "# sketch1, normal1 = create_sketch(img, transforms1)\n",
    "# sketch2, normal2 = create_sketch(img, transforms2)\n",
    "# sketch3, normal3 = create_sketch(img, transforms3)\n",
    "sketch4, normal4 = create_sketch(img, transforms4)\n",
    "\n",
    "plot_images(\n",
    "    [sketch4, normal4],\n",
    "    size=16,\n",
    ")\n",
    "# plot_images(\n",
    "#     [sketch1, normal1, sketch2, normal2, sketch3, normal3, sketch4, normal4],\n",
    "#     size=32,\n",
    "# )\n",
    "# plot_images(\n",
    "#     [sketch1, sketch2, sketch3, sketch4],\n",
    "#     size=32,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = metainfo.load_sketch(metainfo.obj_ids[2], \"00011\")\n",
    "sketch, normal = create_sketch(img, transforms=transforms2)\n",
    "plot_images([sketch, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.load_normal(metainfo.obj_ids[4008], \"00011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = metainfo.load_sketch(metainfo.obj_ids[4112], \"00011\")\n",
    "transforms = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "trans = BaseTransform(transforms=transforms)\n",
    "to_image = BaseTransform(normalize=False, transforms=transforms)\n",
    "sketch = trans(img)[None, ...].to(\"cuda\")\n",
    "model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "# model.deepsdf.hparams[\"surface_eps\"] = 5e-02\n",
    "model.deepsdf.hparams[\"surface_eps\"] = 1e-03\n",
    "model.deepsdf.create_camera(azim=0, elev=-0)\n",
    "\n",
    "rendered_normal = model.capture_camera_frame(\"grayscale\").detach().cpu().numpy()\n",
    "\n",
    "model.deepsdf.eval()\n",
    "with torch.no_grad():\n",
    "    points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "        latent=model.latent,\n",
    "        points=model.deepsdf.camera_points,\n",
    "        mask=model.deepsdf.camera_mask,\n",
    "        rays=model.deepsdf.camera_rays,\n",
    "    )\n",
    "    image = model.deepsdf.render_grayscale(\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        mask=surface_mask,\n",
    "    )\n",
    "rendered_normal = image.detach().cpu().numpy()\n",
    "sketch, normal = to_image(img).permute(1, 2, 0), rendered_normal\n",
    "plot_images([sketch, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 50\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 10\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = model.deepsdf.render_normals(points, model.latent, model.deepsdf.camera_mask)\n",
    "plot_images(normals.detach().cpu().numpy())\n",
    "normals = (normals - 0.5) * 2  # IMPORTANT transform back to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_points = (\n",
    "    points.reshape(256, 256, 3) - normals * min_sdf.reshape(256, 256)[..., None]\n",
    ")\n",
    "(normals * min_sdf.reshape(256, 256)[..., None])[surface_mask.reshape(256, 256)]\n",
    "intrinsic = torch.tensor(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ").to(proj_points)\n",
    "p = proj_points @ model.deepsdf.world_to_camera[:3, :3].T.float()\n",
    "# p = (p @ intrinsic.T)[:, :, :2]\n",
    "\n",
    "# i = np.zeros((256, 256))\n",
    "# for x in tqdm(range(256)):\n",
    "#     for y in range(256):\n",
    "#         px = (p[:, :, 0] >= x) & (p[:, :, 0] <= x+1)\n",
    "#         py = (p[:, :, 1] >= y) & (p[:, :, 1] <= y+1)\n",
    "#         hit = bool((px & py).sum())\n",
    "#         if hit:\n",
    "#             i[x,y] = 1.0\n",
    "\n",
    "from lib.render.camera import Camera\n",
    "\n",
    "camera = Camera()\n",
    "camera.get_camera_to_world() @ np.array([1, 0, 4, 1])\n",
    "intrinsic = np.array(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "intrinsic @ np.array([0, 0, 4])\n",
    "256 * 0.5 - 128\n",
    "# (0 - 256 * 0.5) / 512\n",
    "# (0 - 256 * 0.5) / 512\n",
    "128 + (512 * 1) / 4, 128 + (512 * 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[120, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.reshape(256, 256, 3)[100, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
