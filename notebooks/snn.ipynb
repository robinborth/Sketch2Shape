{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.config import load_config\n",
    "from lib.data.sampler import ChunkSampler\n",
    "from lib.data.metainfo import MetaInfo\n",
    "\n",
    "cfg = load_config(\n",
    "    \"train_loss\", [\"+experiment/train_loss=latent_encoder_shapenet_chair_4096\"]\n",
    ")\n",
    "metainfo = MetaInfo(cfg.data.data_dir, split=\"val_latent\")\n",
    "metainfo.load_loss()\n",
    "sampler = ChunkSampler(metainfo.loss_labels, chunk_size=1)\n",
    "max([i for i in sampler]), len([i for i in sampler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.data.metainfo import MetaInfo\n",
    "from lib.data.transforms import BaseTransform\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from lib.visualize.image import image_grid\n",
    "from torch.nn.functional import l1_loss\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "def transform(normal):\n",
    "    _transform = BaseTransform()\n",
    "    return _transform(normal).to(\"cuda\")\n",
    "\n",
    "\n",
    "def plot_images(images, size: int = 4):\n",
    "    if isinstance(images, list):\n",
    "        _, axes = plt.subplots(1, len(images), figsize=(size, size))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(size, size))\n",
    "        plt.imshow(images)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def siamese_loss(emb_1, emb_2):\n",
    "    return 1 - cosine_similarity(emb_1, emb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"optimize_sketch\", [\"dataset=shapenet_chair_single_view_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[4015]\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\"\n",
    "cfg.model.latent_init = \"retrieval\"\n",
    "cfg.model.retrieval_k = 4\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "\n",
    "sketch_3 = np.asarray(metainfo.load_image(3, 11, 0))\n",
    "sketch_5 = np.asarray(metainfo.load_image(5, 11, 0))\n",
    "rendered_normal_3 = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "rendered_normal_3_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "sketch_3_emb = model.loss(transform(sketch_3)[None, ...])\n",
    "sketch_5_emb = model.loss(transform(sketch_5)[None, ...])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_3_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_3, rendered_normal_3])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_5_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_5, rendered_normal_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.retrieval_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis of Possible Rendering Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_id = 4009\n",
    "\n",
    "gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "model.latent = sketch_emb\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "model.latent = gt_emb\n",
    "gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "# loss_gt_sketch = l1_loss(gt_emb, sketch_emb).item()\n",
    "loss_sketch_normal = l1_loss(sketch_emb, rendered_normal_emb).item()\n",
    "loss_sketch_gt_normal = l1_loss(sketch_emb, gt_rendered_normal_emb).item()\n",
    "# print(f\"loss_gt_sketch={loss_gt_sketch:.4f}\")\n",
    "print(f\"loss_sketch_normal={loss_sketch_normal:.4f}\")\n",
    "print(f\"loss_sketch_gt_normal={loss_sketch_gt_normal:.4f}\")\n",
    "plot_images([sketch, rendered_normal, gt_rendered_normal], size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Analysis Of Possible Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "loss_gt_sketch = []\n",
    "loss_sketch_normal = []\n",
    "loss_sketch_gt_normal = []\n",
    "for shape_id in tqdm(range(3968, 3968 + 128, 1)):  # validation split\n",
    "    # for shape_id in tqdm(range(256)):  # train split\n",
    "    gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "    sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "    sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "    model.latent = sketch_emb\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "    model.latent = gt_emb\n",
    "    gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "    loss_gt_sketch.append(l1_loss(gt_emb, sketch_emb).item())\n",
    "    loss_sketch_normal.append(l1_loss(sketch_emb, rendered_normal_emb).item())\n",
    "    loss_sketch_gt_normal.append(l1_loss(sketch_emb, gt_rendered_normal_emb).item())\n",
    "\n",
    "mean_loss_gt_sketch = np.array(loss_gt_sketch).mean()\n",
    "mean_loss_sketch_normal = np.array(loss_sketch_normal).mean()\n",
    "mean_loss_sketch_gt_normal = np.array(loss_sketch_gt_normal).mean()\n",
    "\n",
    "print(f\"mean_loss_gt_sketch={mean_loss_gt_sketch:.4f}\")\n",
    "print(f\"mean_loss_sketch_normal={mean_loss_sketch_normal:.4f}\")\n",
    "print(f\"mean_loss_sketch_gt_normal={mean_loss_sketch_gt_normal:.4f}\")\n",
    "print(f\"mean_gain={(mean_loss_sketch_normal-mean_loss_sketch_gt_normal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Closest Latent Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.norm(\n",
    "    model.deepsdf.lat_vecs.weight[0] - model.deepsdf.lat_vecs.weight, dim=-1\n",
    ")\n",
    "torch.argsort(idxs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = model.shape_latents.std(0)\n",
    "mean = model.shape_latents.mean(0)\n",
    "loss = []\n",
    "images = []\n",
    "for idx in model.shape_idx:\n",
    "    reg_loss = torch.nn.functional.l1_loss(\n",
    "        model.deepsdf.lat_vecs.weight[idx], model.deepsdf.lat_vecs.weight[2]\n",
    "    )\n",
    "    model.latent = model.deepsdf.lat_vecs.weight[idx]\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(rendered_normal)\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "    loss.append(reg_loss.mean().item())\n",
    "    print(idx, reg_loss.mean())\n",
    "mean_loss = np.array(loss).mean()\n",
    "print(f\"mean_loss={mean_loss:.04}\")\n",
    "for l in loss:\n",
    "    print(f\"{l:.03}\")\n",
    "plot_images(images, size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[3]\n",
    "model.deepsdf.create_camera(azim=40, elev=-30)\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "mean = rendered_normal.mean(-1)\n",
    "mean = np.stack([mean, mean, mean], axis=-1)\n",
    "rendered_gray_emb = model.loss(transform(mean)[None, ...])\n",
    "loss = siamese_loss(rendered_gray_emb, sketch_3_emb)\n",
    "print(loss)\n",
    "plot_images(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative/Qualitative Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "# start_latent_ids = [3, 3385, 2801, 1962, 1058, 782, 1328]  # couch nn\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [0, 5]  # couch far\n",
    "end_latent_id = 0\n",
    "sketch_view = 11\n",
    "traversal_steps = 20\n",
    "image_skip = 2\n",
    "azims = [40]\n",
    "elevs = [-30]\n",
    "\n",
    "# fetch and encode the sketch\n",
    "sketch = metainfo.load_image(end_latent_id, sketch_view, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectories = []\n",
    "loss_trajectories = []\n",
    "for idx, start_latent_id in enumerate(start_latent_ids):\n",
    "    image_trajectory = [sketch]\n",
    "    loss_trajectory = []\n",
    "    desc = f\"{idx+1}/{len(start_latent_ids)}\"\n",
    "    for t in tqdm(np.linspace(1, 0, traversal_steps), desc=desc):\n",
    "        start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "        end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "\n",
    "        # calculate the mean loss from all the views\n",
    "        loss = []\n",
    "        for azim in azims:\n",
    "            for elev in elevs:\n",
    "                model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "                rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "                # mean = rendered_normal.mean(-1)\n",
    "                # rendered_normal = np.stack([mean, mean,mean], axis=-1)\n",
    "                rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])\n",
    "                snn_loss = siamese_loss(sketch_emb, rendered_normal_emb)\n",
    "                loss.append(snn_loss)\n",
    "                image_trajectory.append(rendered_normal)\n",
    "        loss = torch.stack(loss).mean()\n",
    "\n",
    "        # add the loss to the trajectory\n",
    "        loss_trajectory.append(loss.detach().cpu().numpy())\n",
    "    loss_trajectories.append(loss_trajectory)\n",
    "    image_trajectories.append(image_trajectory)\n",
    "\n",
    "# plot the images\n",
    "for image_trajectory in image_trajectories:\n",
    "    trajectory = []\n",
    "    for idx, img in enumerate(image_trajectory):\n",
    "        if idx == 0 or (idx - 1) % image_skip == 0:\n",
    "            trajectory.append(img)\n",
    "    plot_images(trajectory, size=16)\n",
    "\n",
    "# plot the loss curves\n",
    "for obj_id, loss_trajectory in zip(start_latent_ids, loss_trajectories):\n",
    "    plt.plot(\n",
    "        np.linspace(0, 1, traversal_steps),\n",
    "        np.stack(loss_trajectory),\n",
    "        label=obj_id,\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "mean = np.stack(loss_trajectories).mean(0)\n",
    "std = np.stack(loss_trajectories).std(0)\n",
    "plt.plot(x, mean)\n",
    "plt.fill_between(x, (mean - std), (mean + std), color=\"b\", alpha=0.1)\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[0]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "normal_emb = model.loss(transform(normal)[None, ...])\n",
    "print(f\"{siamese_loss(sketch_3_emb, normal_emb)=}\")\n",
    "plot_images([sketch_3, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensor(2549, device='cuda:0') tensor(0.6049, device='cuda:0')\n",
    "tensor(1328, device='cuda:0') tensor(1.4948, device='cuda:0')\n",
    "tensor(1898, device='cuda:0') tensor(0.6908, device='cuda:0')\n",
    "tensor(962, device='cuda:0') tensor(0.6423, device='cuda:0')\n",
    "tensor(535, device='cuda:0') tensor(0.8771, device='cuda:0')\n",
    "tensor(755, device='cuda:0') tensor(0.4972, device='cuda:0')\n",
    "tensor(2432, device='cuda:0') tensor(1.8280, device='cuda:0')\n",
    "tensor(3089, device='cuda:0') tensor(0.9243, device='cuda:0')\n",
    "tensor(3885, device='cuda:0') tensor(1.6862, device='cuda:0')\n",
    "tensor(1447, device='cuda:0') tensor(1.0207, device='cuda:0')\n",
    "tensor(3195, device='cuda:0') tensor(0.8885, device='cuda:0')\n",
    "tensor(2207, device='cuda:0') tensor(0.7391, device='cuda:0')\n",
    "tensor(277, device='cuda:0') tensor(0.7135, device='cuda:0')\n",
    "tensor(1058, device='cuda:0') tensor(0.6626, device='cuda:0')\n",
    "tensor(841, device='cuda:0') tensor(1.0481, device='cuda:0')\n",
    "tensor(1637, device='cuda:0') tensor(0.6819, device='cuda:0')\n",
    "\"\"\"\n",
    "\n",
    "model.latent = model.deepsdf.lat_vecs.weight[3885]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "\n",
    "mean = model.shape_latents.mean(0)\n",
    "std = model.shape_latents.std(0)\n",
    "\n",
    "good_latents = []\n",
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0 and i != 3:\n",
    "        good_latents.append(latent)\n",
    "\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"init\", i, reg_loss)\n",
    "\n",
    "mean = torch.stack(good_latents).mean(0)\n",
    "std = torch.stack(good_latents).std(0)\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"good\", i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0:\n",
    "        print(i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[1962]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "# mean = model.shape_latents.mean(0)\n",
    "# std = model.shape_latents.std(0)\n",
    "# mean = torch.stack(good_latents).mean(0)\n",
    "# std = torch.stack(good_latents).std(0)\n",
    "\n",
    "((mean - latent) / std).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Latent Code (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_id = 3385\n",
    "end_latent_id = 3\n",
    "optim_steps = 10\n",
    "\n",
    "t = torch.tensor(1.0, dtype=torch.float32).to(\"cuda\")\n",
    "t.requires_grad = True\n",
    "start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "start_latent.requires_grad = True\n",
    "end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "end_latent.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([t], lr=0.1)\n",
    "\n",
    "sketch = metainfo.load_image(end_latent_id, 11, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectory = [np.asarray(sketch)]\n",
    "loss_trajectory = []\n",
    "reg_loss_trajectory = []\n",
    "t_trajectory = []\n",
    "with tqdm(total=optim_steps) as pbar:\n",
    "    for step in range(optim_steps):\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "        points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "            latent=model.latent,\n",
    "            points=model.deepsdf.camera_points,\n",
    "            rays=model.deepsdf.camera_rays,\n",
    "            mask=model.deepsdf.camera_mask,\n",
    "        )\n",
    "        rendered_normal = model.deepsdf.render_normals(\n",
    "            latent=model.latent,\n",
    "            points=points,\n",
    "            mask=surface_mask,\n",
    "        )  # (H, W, 3)\n",
    "        normal = model.deepsdf.image_to_siamese(rendered_normal)  # (1, 3, H, W)\n",
    "        normal_emb = model.loss(normal)  # (1, D)\n",
    "\n",
    "        snn_loss = siamese_loss(sketch_emb, normal_emb)\n",
    "\n",
    "        std = model.shape_latents.std(0)\n",
    "        mean = model.shape_latents.mean(0)\n",
    "        reg_loss = ((model.latent.clone() - mean) / std).pow(2)\n",
    "        reg_loss_trajectory.append(reg_loss.mean().item())\n",
    "\n",
    "        loss_trajectory.append(snn_loss.item())\n",
    "        t_trajectory.append(t.item())\n",
    "        image_trajectory.append(rendered_normal.detach().cpu().numpy())\n",
    "\n",
    "        snn_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_postfix({\"t\": t.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "# images\n",
    "plot_images(image_trajectory, size=16)\n",
    "\n",
    "# loss\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(reg_loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"reg_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(t_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random from Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_latent_ids = [0, 3176, 2110, 2733, 2317, 94, 3831]  # chair nn\n",
    "mean = model.deepsdf.lat_vecs.weight[5]\n",
    "std = model.deepsdf.lat_vecs.weight.std(0)\n",
    "latent = mean + torch.randn_like(std) * std * 0.3\n",
    "print(l1_loss(latent, mean))\n",
    "model.latent = latent\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "start_latent_id = 1\n",
    "mean = model.deepsdf.lat_vecs.weight[0]\n",
    "images = []\n",
    "for t in np.linspace(0, 1, 11):\n",
    "    latent = t * model.deepsdf.lat_vecs.weight[start_latent_id] + (1 - t) * mean\n",
    "    model.latent = latent\n",
    "    normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(normal)\n",
    "plot_images(images, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sketch(img):\n",
    "    img = cv.Canny(\n",
    "        (img * 255).astype(np.uint8),\n",
    "        100,\n",
    "        150,\n",
    "        L2gradient=True,\n",
    "        apertureSize=3,\n",
    "    )\n",
    "    img = cv.bitwise_not(img)\n",
    "    return np.stack([img, img, img], axis=-1).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "# for i in torch.randint(4096, (100,)):\n",
    "for i in [0] * 20:\n",
    "    t = torch.normal(torch.tensor(0.25), torch.tensor(0.1))\n",
    "    t = torch.clamp(t, 0.0, 0.5)\n",
    "    # azim = torch.normal(torch.tensor(30.0), torch.tensor(5))\n",
    "    # azim = torch.normal(torch.tensor(45.0), torch.tensor(22.5))\n",
    "    azim = torch.normal(torch.tensor(90.0), torch.tensor(22.5))\n",
    "    elev = torch.normal(torch.tensor(-20.0), torch.tensor(5))\n",
    "    model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "    print(f\"{t=}\")\n",
    "    print(f\"{azim=}\")\n",
    "    print(f\"{elev=}\")\n",
    "    source_id = 0\n",
    "    target_id = i\n",
    "\n",
    "    source_latent = model.deepsdf.lat_vecs.weight[source_id]\n",
    "    model.latent = source_latent\n",
    "    source_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    target_latent = model.deepsdf.lat_vecs.weight[target_id]\n",
    "    model.latent = target_latent\n",
    "    target_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    interpolated_latent = (1 - t) * source_latent + t * target_latent\n",
    "    model.latent = interpolated_latent\n",
    "    interpolated_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    print(l1_loss(source_latent, target_latent))\n",
    "    plot_images(\n",
    "        [\n",
    "            source_normal,\n",
    "            target_normal,\n",
    "            interpolated_normal,\n",
    "            to_sketch(interpolated_normal),\n",
    "        ],\n",
    "        size=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.preprocess import PreprocessRenderings\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "preprocess = PreprocessRenderings(\n",
    "    data_dir=\"/home/borth/sketch2shape/data/shapenet_chair_4096\",\n",
    "    deepsdf_ckpt_path=\"/home/borth/sketch2shape/checkpoints/deepsdf.ckpt\",\n",
    "    n_renderings=10,\n",
    ")\n",
    "normals, sketches, latents, config = preprocess.preprocess(obj_id=metainfo.obj_ids[17])\n",
    "plot_images(normals, size=16)\n",
    "plot_images(sketches, size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandDrawn Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lib.data.transforms import BaseTransform, ToGrayScale, DilateSketch, ToSketch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[0]\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_multi_view_256.ckpt\"\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\"\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_64.ckpt\"\n",
    "# cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_128.ckpt\"\n",
    "\n",
    "cfg.model.latent_init = \"latent\"\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "rendered_normal_latent = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "# path = \"/home/borth/sketch2shape/temp/chair1.png\"\n",
    "# img = Image.open(path).convert(\"RGB\")\n",
    "# path = \"/home/borth/sketch2shape/temp/chair2.png\"\n",
    "# img = Image.open(path).convert(\"RGB\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4117], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4112], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4126], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4130], \"00011\")\n",
    "# img = metainfo.load_sketch(metainfo.obj_ids[4105], \"00011\")\n",
    "img = metainfo.load_sketch(metainfo.obj_ids[4107], \"00011\")\n",
    "img = metainfo.load_sketch(metainfo.obj_ids[4108], \"00011\")\n",
    "\n",
    "transforms1 = [v2.Resize((256, 256))]\n",
    "transforms2 = [v2.Resize((256, 256)), ToSketch()]\n",
    "transforms3 = [v2.Resize((256, 256)), DilateSketch(kernel_size=5)]\n",
    "transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "\n",
    "# transforms1 = [v2.Resize((256, 256)), v2.Resize((64, 64), antialias=True)]\n",
    "# transforms2 = [v2.Resize((256, 256)), ToSketch(),v2.Resize((64, 64), antialias=True)]\n",
    "# transforms3 = [v2.Resize((256, 256), antialias=True), DilateSketch(kernel_size=5),v2.Resize((64, 64), antialias=True)]\n",
    "# transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5),v2.Resize((64, 64), antialias=True)]\n",
    "\n",
    "# transforms1 = [v2.Resize((256, 256)), v2.Resize((128, 128), antialias=True)]\n",
    "# transforms2 = [v2.Resize((256, 256)), ToSketch(),v2.Resize((128, 128), antialias=True)]\n",
    "# transforms3 = [v2.Resize((256, 256), antialias=True), DilateSketch(kernel_size=5),v2.Resize((128, 128), antialias=True)]\n",
    "# transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5),v2.Resize((128, 128), antialias=True)]\n",
    "\n",
    "\n",
    "def create_sketch(image, transforms):\n",
    "    trans = BaseTransform(transforms=transforms)\n",
    "    to_image = BaseTransform(normalize=False, transforms=transforms)\n",
    "    sketch = trans(image)[None, ...].to(\"cuda\")\n",
    "    model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "    # model.deepsdf.hparams[\"surface_eps\"] = 5e-02\n",
    "    model.deepsdf.hparams[\"surface_eps\"] = 1e-03\n",
    "    # model.deepsdf.create_camera(azim=60, elev=-25)\n",
    "    # model.deepsdf.create_camera(azim=120, elev=-10)\n",
    "    model.deepsdf.create_camera(azim=40, elev=-20)\n",
    "    rendered_normal = model.capture_camera_frame(\"grayscale\").detach().cpu().numpy()\n",
    "    return to_image(image).permute(1, 2, 0), rendered_normal\n",
    "\n",
    "\n",
    "# sketch1, normal1 = create_sketch(img, transforms1)\n",
    "# sketch2, normal2 = create_sketch(img, transforms2)\n",
    "# sketch3, normal3 = create_sketch(img, transforms3)\n",
    "sketch4, normal4 = create_sketch(img, transforms4)\n",
    "\n",
    "plot_images(\n",
    "    [sketch4, normal4],\n",
    "    size=16,\n",
    ")\n",
    "# plot_images(\n",
    "#     [sketch1, normal1, sketch2, normal2, sketch3, normal3, sketch4, normal4],\n",
    "#     size=32,\n",
    "# )\n",
    "# plot_images(\n",
    "#     [sketch1, sketch2, sketch3, sketch4],\n",
    "#     size=32,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = metainfo.load_sketch(metainfo.obj_ids[2], \"00011\")\n",
    "sketch, normal = create_sketch(img, transforms=transforms2)\n",
    "plot_images([sketch, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metainfo.load_normal(metainfo.obj_ids[4008], \"00011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = metainfo.load_sketch(metainfo.obj_ids[4112], \"00011\")\n",
    "transforms = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "trans = BaseTransform(transforms=transforms)\n",
    "to_image = BaseTransform(normalize=False, transforms=transforms)\n",
    "sketch = trans(img)[None, ...].to(\"cuda\")\n",
    "model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "# model.deepsdf.hparams[\"surface_eps\"] = 5e-02\n",
    "model.deepsdf.hparams[\"surface_eps\"] = 1e-03\n",
    "model.deepsdf.create_camera(azim=0, elev=-0)\n",
    "\n",
    "rendered_normal = model.capture_camera_frame(\"grayscale\").detach().cpu().numpy()\n",
    "\n",
    "model.deepsdf.eval()\n",
    "with torch.no_grad():\n",
    "    points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "        latent=model.latent,\n",
    "        points=model.deepsdf.camera_points,\n",
    "        mask=model.deepsdf.camera_mask,\n",
    "        rays=model.deepsdf.camera_rays,\n",
    "    )\n",
    "    image = model.deepsdf.render_grayscale(\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        mask=surface_mask,\n",
    "    )\n",
    "rendered_normal = image.detach().cpu().numpy()\n",
    "sketch, normal = to_image(img).permute(1, 2, 0), rendered_normal\n",
    "plot_images([sketch, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 50\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 10\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = model.deepsdf.render_normals(points, model.latent, model.deepsdf.camera_mask)\n",
    "plot_images(normals.detach().cpu().numpy())\n",
    "normals = (normals - 0.5) * 2  # IMPORTANT transform back to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_points = (\n",
    "    points.reshape(256, 256, 3) - normals * min_sdf.reshape(256, 256)[..., None]\n",
    ")\n",
    "(normals * min_sdf.reshape(256, 256)[..., None])[surface_mask.reshape(256, 256)]\n",
    "intrinsic = torch.tensor(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ").to(proj_points)\n",
    "p = proj_points @ model.deepsdf.world_to_camera[:3, :3].T.float()\n",
    "# p = (p @ intrinsic.T)[:, :, :2]\n",
    "\n",
    "# i = np.zeros((256, 256))\n",
    "# for x in tqdm(range(256)):\n",
    "#     for y in range(256):\n",
    "#         px = (p[:, :, 0] >= x) & (p[:, :, 0] <= x+1)\n",
    "#         py = (p[:, :, 1] >= y) & (p[:, :, 1] <= y+1)\n",
    "#         hit = bool((px & py).sum())\n",
    "#         if hit:\n",
    "#             i[x,y] = 1.0\n",
    "\n",
    "from lib.render.camera import Camera\n",
    "\n",
    "camera = Camera()\n",
    "camera.get_camera_to_world() @ np.array([1, 0, 4, 1])\n",
    "intrinsic = np.array(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "intrinsic @ np.array([0, 0, 4])\n",
    "256 * 0.5 - 128\n",
    "# (0 - 256 * 0.5) / 512\n",
    "# (0 - 256 * 0.5) / 512\n",
    "128 + (512 * 1) / 4, 128 + (512 * 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[120, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.reshape(256, 256, 3)[100, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
