{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.config import load_config\n",
    "from lib.data.sampler import ChunkSampler\n",
    "from lib.data.metainfo import MetaInfo\n",
    "\n",
    "cfg = load_config(\n",
    "    \"train_loss\", [\"+experiment/train_loss=latent_encoder_shapenet_chair_4096\"]\n",
    ")\n",
    "metainfo = MetaInfo(cfg.data.data_dir, split=\"val_latent\")\n",
    "metainfo.load_loss()\n",
    "sampler = ChunkSampler(metainfo.loss_labels, chunk_size=1)\n",
    "max([i for i in sampler]), len([i for i in sampler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.data.metainfo import MetaInfo\n",
    "from lib.data.transforms import BaseTransform\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from lib.visualize.image import image_grid\n",
    "from torch.nn.functional import l1_loss\n",
    "import cv2 as cv\n",
    "\n",
    "def transform(normal):\n",
    "    _transform = BaseTransform()\n",
    "    return _transform(normal).to(\"cuda\")\n",
    "\n",
    "\n",
    "def plot_images(images, size: int = 4):\n",
    "    if isinstance(images, list):\n",
    "        _, axes = plt.subplots(1, len(images), figsize=(size, size))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(size, size))\n",
    "        plt.imshow(images)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def siamese_loss(emb_1, emb_2):\n",
    "    return 1 - cosine_similarity(emb_1, emb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[0]\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_edge_grayscale_multi_view_256.ckpt\"\n",
    "cfg.model.latent_init = \"latent\"\n",
    "cfg.model.retrieval_k = 4\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "\n",
    "sketch_3 = np.asarray(metainfo.load_image(3, 11, 0))\n",
    "sketch_5 = np.asarray(metainfo.load_image(5, 11, 0))\n",
    "rendered_normal_3 = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "rendered_normal_3_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "sketch_3_emb = model.loss(transform(sketch_3)[None, ...])\n",
    "sketch_5_emb = model.loss(transform(sketch_5)[None, ...])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_3_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_3, rendered_normal_3])\n",
    "\n",
    "print(f\"{siamese_loss(sketch_5_emb, rendered_normal_3_emb)=}\")\n",
    "plot_images([sketch_5, rendered_normal_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Analysis of Possible Rendering Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shape_id = 4009\n",
    "\n",
    "gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "model.latent = sketch_emb\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "model.latent = gt_emb\n",
    "gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "# loss_gt_sketch = l1_loss(gt_emb, sketch_emb).item()\n",
    "loss_sketch_normal = l1_loss(sketch_emb, rendered_normal_emb).item()\n",
    "loss_sketch_gt_normal = l1_loss(sketch_emb, gt_rendered_normal_emb).item()\n",
    "# print(f\"loss_gt_sketch={loss_gt_sketch:.4f}\")\n",
    "print(f\"loss_sketch_normal={loss_sketch_normal:.4f}\")\n",
    "print(f\"loss_sketch_gt_normal={loss_sketch_gt_normal:.4f}\")\n",
    "plot_images([sketch, rendered_normal, gt_rendered_normal], size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Analysis Of Possible Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "loss_gt_sketch = []\n",
    "loss_sketch_normal = []\n",
    "loss_sketch_gt_normal = []\n",
    "for shape_id in tqdm(range(3968, 3968 + 128, 1)):  # validation split\n",
    "    # for shape_id in tqdm(range(256)):  # train split\n",
    "    gt_emb = model.deepsdf.lat_vecs.weight[shape_id]\n",
    "    sketch = np.asarray(metainfo.load_image(shape_id, 11, 0))\n",
    "    sketch_emb = model.loss(transform(sketch)[None, ...])[0]\n",
    "    model.latent = sketch_emb\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])[0]\n",
    "\n",
    "    model.latent = gt_emb\n",
    "    gt_rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    gt_rendered_normal_emb = model.loss(transform(gt_rendered_normal)[None, ...])[0]\n",
    "\n",
    "    loss_gt_sketch.append(l1_loss(gt_emb, sketch_emb).item())\n",
    "    loss_sketch_normal.append(l1_loss(sketch_emb, rendered_normal_emb).item())\n",
    "    loss_sketch_gt_normal.append(l1_loss(sketch_emb, gt_rendered_normal_emb).item())\n",
    "\n",
    "mean_loss_gt_sketch = np.array(loss_gt_sketch).mean()\n",
    "mean_loss_sketch_normal = np.array(loss_sketch_normal).mean()\n",
    "mean_loss_sketch_gt_normal = np.array(loss_sketch_gt_normal).mean()\n",
    "\n",
    "print(f\"mean_loss_gt_sketch={mean_loss_gt_sketch:.4f}\")\n",
    "print(f\"mean_loss_sketch_normal={mean_loss_sketch_normal:.4f}\")\n",
    "print(f\"mean_loss_sketch_gt_normal={mean_loss_sketch_gt_normal:.4f}\")\n",
    "print(f\"mean_gain={(mean_loss_sketch_normal-mean_loss_sketch_gt_normal):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Closest Latent Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = torch.norm(\n",
    "    model.deepsdf.lat_vecs.weight[0] - model.deepsdf.lat_vecs.weight, dim=-1\n",
    ")\n",
    "torch.argsort(idxs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = model.shape_latents.std(0)\n",
    "mean = model.shape_latents.mean(0)\n",
    "loss = []\n",
    "images = []\n",
    "for idx in model.shape_idx:\n",
    "    reg_loss = torch.nn.functional.l1_loss(\n",
    "        model.deepsdf.lat_vecs.weight[idx], model.deepsdf.lat_vecs.weight[2]\n",
    "    )\n",
    "    model.latent = model.deepsdf.lat_vecs.weight[idx]\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(rendered_normal)\n",
    "    rendered_normal_emb = model.loss(transform(rendered_normal_3)[None, ...])\n",
    "    loss.append(reg_loss.mean().item())\n",
    "    print(idx, reg_loss.mean())\n",
    "mean_loss = np.array(loss).mean()\n",
    "print(f\"mean_loss={mean_loss:.04}\")\n",
    "for l in loss:\n",
    "    print(f\"{l:.03}\")\n",
    "plot_images(images, size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gray Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[3]\n",
    "model.deepsdf.create_camera(azim=40, elev=-30)\n",
    "rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "mean = rendered_normal.mean(-1)\n",
    "mean = np.stack([mean, mean, mean], axis=-1)\n",
    "rendered_gray_emb = model.loss(transform(mean)[None, ...])\n",
    "loss = siamese_loss(rendered_gray_emb, sketch_3_emb)\n",
    "print(loss)\n",
    "plot_images(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative/Qualitative Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "# start_latent_ids = [3, 3385, 2801, 1962, 1058, 782, 1328]  # couch nn\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [3, 0, 1, 2, 4, 5, 6] # couch far\n",
    "# start_latent_ids = [0, 5]  # couch far\n",
    "end_latent_id = 0\n",
    "sketch_view = 11\n",
    "traversal_steps = 20\n",
    "image_skip = 2\n",
    "azims = [40]\n",
    "elevs = [-30]\n",
    "\n",
    "# fetch and encode the sketch\n",
    "sketch = metainfo.load_image(end_latent_id, sketch_view, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectories = []\n",
    "loss_trajectories = []\n",
    "for idx, start_latent_id in enumerate(start_latent_ids):\n",
    "    image_trajectory = [sketch]\n",
    "    loss_trajectory = []\n",
    "    desc = f\"{idx+1}/{len(start_latent_ids)}\"\n",
    "    for t in tqdm(np.linspace(1, 0, traversal_steps), desc=desc):\n",
    "        start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "        end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "\n",
    "        # calculate the mean loss from all the views\n",
    "        loss = []\n",
    "        for azim in azims:\n",
    "            for elev in elevs:\n",
    "                model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "                rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "                # mean = rendered_normal.mean(-1)\n",
    "                # rendered_normal = np.stack([mean, mean,mean], axis=-1)\n",
    "                rendered_normal_emb = model.loss(transform(rendered_normal)[None, ...])\n",
    "                snn_loss = siamese_loss(sketch_emb, rendered_normal_emb)\n",
    "                loss.append(snn_loss)\n",
    "                image_trajectory.append(rendered_normal)\n",
    "        loss = torch.stack(loss).mean()\n",
    "\n",
    "        # add the loss to the trajectory\n",
    "        loss_trajectory.append(loss.detach().cpu().numpy())\n",
    "    loss_trajectories.append(loss_trajectory)\n",
    "    image_trajectories.append(image_trajectory)\n",
    "\n",
    "# plot the images\n",
    "for image_trajectory in image_trajectories:\n",
    "    trajectory = []\n",
    "    for idx, img in enumerate(image_trajectory):\n",
    "        if idx == 0 or (idx - 1) % image_skip == 0:\n",
    "            trajectory.append(img)\n",
    "    plot_images(trajectory, size=16)\n",
    "\n",
    "# plot the loss curves\n",
    "for obj_id, loss_trajectory in zip(start_latent_ids, loss_trajectories):\n",
    "    plt.plot(\n",
    "        np.linspace(0, 1, traversal_steps),\n",
    "        np.stack(loss_trajectory),\n",
    "        label=obj_id,\n",
    "    )\n",
    "    plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "mean = np.stack(loss_trajectories).mean(0)\n",
    "std = np.stack(loss_trajectories).std(0)\n",
    "plt.plot(x, mean)\n",
    "plt.fill_between(x, (mean - std), (mean + std), color=\"b\", alpha=0.1)\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"traversal_steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[0]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "normal_emb = model.loss(transform(normal)[None, ...])\n",
    "print(f\"{siamese_loss(sketch_3_emb, normal_emb)=}\")\n",
    "plot_images([sketch_3, normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tensor(2549, device='cuda:0') tensor(0.6049, device='cuda:0')\n",
    "tensor(1328, device='cuda:0') tensor(1.4948, device='cuda:0')\n",
    "tensor(1898, device='cuda:0') tensor(0.6908, device='cuda:0')\n",
    "tensor(962, device='cuda:0') tensor(0.6423, device='cuda:0')\n",
    "tensor(535, device='cuda:0') tensor(0.8771, device='cuda:0')\n",
    "tensor(755, device='cuda:0') tensor(0.4972, device='cuda:0')\n",
    "tensor(2432, device='cuda:0') tensor(1.8280, device='cuda:0')\n",
    "tensor(3089, device='cuda:0') tensor(0.9243, device='cuda:0')\n",
    "tensor(3885, device='cuda:0') tensor(1.6862, device='cuda:0')\n",
    "tensor(1447, device='cuda:0') tensor(1.0207, device='cuda:0')\n",
    "tensor(3195, device='cuda:0') tensor(0.8885, device='cuda:0')\n",
    "tensor(2207, device='cuda:0') tensor(0.7391, device='cuda:0')\n",
    "tensor(277, device='cuda:0') tensor(0.7135, device='cuda:0')\n",
    "tensor(1058, device='cuda:0') tensor(0.6626, device='cuda:0')\n",
    "tensor(841, device='cuda:0') tensor(1.0481, device='cuda:0')\n",
    "tensor(1637, device='cuda:0') tensor(0.6819, device='cuda:0')\n",
    "\"\"\"\n",
    "\n",
    "model.latent = model.deepsdf.lat_vecs.weight[3885]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "\n",
    "mean = model.shape_latents.mean(0)\n",
    "std = model.shape_latents.std(0)\n",
    "\n",
    "good_latents = []\n",
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0 and i != 3:\n",
    "        good_latents.append(latent)\n",
    "\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"init\", i, reg_loss)\n",
    "\n",
    "mean = torch.stack(good_latents).mean(0)\n",
    "std = torch.stack(good_latents).std(0)\n",
    "for i in range(5):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    print(\"good\", i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4096):\n",
    "    latent = model.deepsdf.lat_vecs.weight[i]\n",
    "    reg_loss = ((mean - latent) / std).pow(2).mean()\n",
    "    if reg_loss <= 1.0:\n",
    "        print(i, reg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.deepsdf.lat_vecs.weight[1962]\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)\n",
    "# mean = model.shape_latents.mean(0)\n",
    "# std = model.shape_latents.std(0)\n",
    "# mean = torch.stack(good_latents).mean(0)\n",
    "# std = torch.stack(good_latents).std(0)\n",
    "\n",
    "((mean - latent) / std).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Latent Code (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_id = 3385\n",
    "end_latent_id = 3\n",
    "optim_steps = 10\n",
    "\n",
    "t = torch.tensor(1.0, dtype=torch.float32).to(\"cuda\")\n",
    "t.requires_grad = True\n",
    "start_latent = model.deepsdf.lat_vecs.weight[start_latent_id]\n",
    "start_latent.requires_grad = True\n",
    "end_latent = model.deepsdf.lat_vecs.weight[end_latent_id]\n",
    "end_latent.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([t], lr=0.1)\n",
    "\n",
    "sketch = metainfo.load_image(end_latent_id, 11, 0)\n",
    "sketch_emb = model.loss(transform(sketch)[None, ...])\n",
    "\n",
    "image_trajectory = [np.asarray(sketch)]\n",
    "loss_trajectory = []\n",
    "reg_loss_trajectory = []\n",
    "t_trajectory = []\n",
    "with tqdm(total=optim_steps) as pbar:\n",
    "    for step in range(optim_steps):\n",
    "        model.latent = t * start_latent + (1 - t) * end_latent\n",
    "        points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "            latent=model.latent,\n",
    "            points=model.deepsdf.camera_points,\n",
    "            rays=model.deepsdf.camera_rays,\n",
    "            mask=model.deepsdf.camera_mask,\n",
    "        )\n",
    "        rendered_normal = model.deepsdf.render_normals(\n",
    "            latent=model.latent,\n",
    "            points=points,\n",
    "            mask=surface_mask,\n",
    "        )  # (H, W, 3)\n",
    "        normal = model.deepsdf.normal_to_siamese(rendered_normal)  # (1, 3, H, W)\n",
    "        normal_emb = model.loss(normal)  # (1, D)\n",
    "\n",
    "        snn_loss = siamese_loss(sketch_emb, normal_emb)\n",
    "\n",
    "        std = model.shape_latents.std(0)\n",
    "        mean = model.shape_latents.mean(0)\n",
    "        reg_loss = ((model.latent.clone() - mean) / std).pow(2)\n",
    "        reg_loss_trajectory.append(reg_loss.mean().item())\n",
    "\n",
    "        loss_trajectory.append(snn_loss.item())\n",
    "        t_trajectory.append(t.item())\n",
    "        image_trajectory.append(rendered_normal.detach().cpu().numpy())\n",
    "\n",
    "        snn_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_postfix({\"t\": t.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "# images\n",
    "plot_images(image_trajectory, size=16)\n",
    "\n",
    "# loss\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"siamese_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(reg_loss_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"reg_loss\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, optim_steps),\n",
    "    np.stack(t_trajectory),\n",
    "    label=obj_id,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.xlabel(\"optim_steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random from Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_latent_ids = [0, 3176, 2110, 2733, 2317, 94, 3831]  # chair nn\n",
    "mean = model.deepsdf.lat_vecs.weight[5]\n",
    "std = model.deepsdf.lat_vecs.weight.std(0)\n",
    "latent = mean + torch.randn_like(std) * std * 0.3\n",
    "print(l1_loss(latent, mean))\n",
    "model.latent = latent\n",
    "normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent_ids = [0, 2317, 189, 837, 2733, 2785, 3928]  # chair nn\n",
    "start_latent_id = 1\n",
    "mean = model.deepsdf.lat_vecs.weight[0]\n",
    "images = []\n",
    "for t in np.linspace(0, 1, 11):\n",
    "    latent = t * model.deepsdf.lat_vecs.weight[start_latent_id] + (1 - t) * mean\n",
    "    model.latent = latent\n",
    "    normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    images.append(normal)\n",
    "plot_images(images, size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def to_sketch(img):\n",
    "    img = cv.Canny(\n",
    "        (img * 255).astype(np.uint8),\n",
    "        100,\n",
    "        150,\n",
    "        L2gradient=True,\n",
    "        apertureSize=3,\n",
    "    )\n",
    "    img = cv.bitwise_not(img)\n",
    "    return np.stack([img, img, img], axis=-1).astype(np.float32) / 255\n",
    "\n",
    "\n",
    "for i in torch.randint(4096, (100,)):\n",
    "    t = torch.normal(torch.tensor(0.25), torch.tensor(0.1))\n",
    "    t = torch.clamp(t, 0.0, 0.5)\n",
    "    azim = torch.normal(torch.tensor(30.0), torch.tensor(5))\n",
    "    elev = torch.normal(torch.tensor(-20.0), torch.tensor(5))\n",
    "    model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "    print(f\"{t=}\")\n",
    "    print(f\"{azim=}\")\n",
    "    print(f\"{elev=}\")\n",
    "    source_id = 0\n",
    "    target_id = i\n",
    "\n",
    "    source_latent = model.deepsdf.lat_vecs.weight[source_id]\n",
    "    model.latent = source_latent\n",
    "    source_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    target_latent = model.deepsdf.lat_vecs.weight[target_id]\n",
    "    model.latent = target_latent\n",
    "    target_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    interpolated_latent = (1 - t) * source_latent + t * target_latent\n",
    "    model.latent = interpolated_latent\n",
    "    interpolated_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "    print(l1_loss(source_latent, target_latent))\n",
    "    plot_images(\n",
    "        [\n",
    "            source_normal,\n",
    "            target_normal,\n",
    "            interpolated_normal,\n",
    "            to_sketch(interpolated_normal),\n",
    "        ],\n",
    "        size=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.preprocess import PreprocessRenderings\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "preprocess = PreprocessRenderings(\n",
    "    data_dir=\"/home/borth/sketch2shape/data/shapenet_chair_4096\",\n",
    "    deepsdf_ckpt_path=\"/home/borth/sketch2shape/checkpoints/deepsdf.ckpt\",\n",
    "    n_renderings=10,\n",
    ")\n",
    "normals, sketches, latents, config = preprocess.preprocess(obj_id=metainfo.obj_ids[17])\n",
    "plot_images(normals, size=16)\n",
    "plot_images(sketches, size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HandDrawn Sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from lib.data.transforms import BaseTransform, ToGrayScale, DilateSketch, ToSketch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[0]\n",
    "cfg.model.loss_ckpt_path = (\n",
    "    \"/home/borth/sketch2shape/checkpoints/latent_siamese_edge_normal_multi_view_256.ckpt\"\n",
    ")\n",
    "cfg.model.latent_init = \"latent\"\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "rendered_normal_latent = model.capture_camera_frame().detach().cpu().numpy()\n",
    "\n",
    "path = \"/home/borth/sketch2shape/temp/chair1.png\"\n",
    "img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "transforms1 = [v2.Resize((256, 256))]\n",
    "transforms2 = [v2.Resize((256, 256)), ToSketch()]\n",
    "transforms3 = [v2.Resize((256, 256), antialias=True), DilateSketch(kernel_size=5)]\n",
    "transforms4 = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "\n",
    "\n",
    "def create_sketch(image, transforms):\n",
    "    trans = BaseTransform(transforms=transforms)\n",
    "    to_image = BaseTransform(normalize=False, transforms=transforms)\n",
    "    sketch = trans(image)[None, ...].to(\"cuda\")\n",
    "    model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "    rendered_normal = model.capture_camera_frame().detach().cpu().numpy()\n",
    "    return  to_image(image).permute(1, 2, 0), rendered_normal\n",
    "\n",
    "\n",
    "sketch1, normal1 = create_sketch(img, transforms1)\n",
    "sketch2, normal2 = create_sketch(img, transforms2)\n",
    "sketch3, normal3 = create_sketch(img, transforms3)\n",
    "sketch4, normal4 = create_sketch(img, transforms4)\n",
    "\n",
    "plot_images(\n",
    "    [sketch1, normal1, sketch2, normal2, sketch3, normal3, sketch4, normal4],\n",
    "    size=32,\n",
    ")\n",
    "plot_images(\n",
    "    [sketch1, sketch2, sketch3, sketch4],\n",
    "    size=32,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
