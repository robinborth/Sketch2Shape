{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.models.latent_encoder import LatentEncoder\n",
    "from lib.models.deepsdf import DeepSDF\n",
    "from torchvision.transforms import v2\n",
    "from lib.utils.config import load_config\n",
    "from lib.data.metainfo import MetaInfo\n",
    "import hydra\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from lib.utils.config import load_config\n",
    "from lib.data.sampler import ChunkSampler\n",
    "from lib.data.metainfo import MetaInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Frames from Raw Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 102 frames from 1 videos\n",
      "Saved 102 frames to disk at '/home/korth/sketch2shape/demo_video/video_frames'\n"
     ]
    }
   ],
   "source": [
    "# capture all .mov videos in raw_video directory\n",
    "video_folder = \"/home/korth/sketch2shape/demo_video\"\n",
    "video_files = glob.glob(f\"{video_folder}/*.mov\")\n",
    "frames_folder = f\"{video_folder}/video_frames\"\n",
    "frames = []\n",
    "\n",
    "num_frames_to_extract = 100  # Set the number of frames to extract\n",
    "for video_file in video_files:\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate the frame interval\n",
    "    frame_interval = max(total_frames // num_frames_to_extract, 1)\n",
    "    \n",
    "    # Read frames at the specified interval\n",
    "    frame_count = 0\n",
    "    while video.isOpened() and frame_count < total_frames:\n",
    "        # Read the current frame\n",
    "        ret, frame = video.read()\n",
    "        \n",
    "        # If the frame was read successfully\n",
    "        if ret:\n",
    "            # Add the frame to the list of frames\n",
    "            if frame_count % frame_interval == 0:\n",
    "                frames.append(frame)\n",
    "            frame_count += 1\n",
    "        else:\n",
    "            # Break the loop if the video is completed\n",
    "            break\n",
    "    \n",
    "    # Release the video file\n",
    "    video.release()\n",
    "\n",
    "print(f\"Extracted {len(frames)} frames from {len(video_files)} videos\")\n",
    "\n",
    "# create folder if it does not exist (in python)\n",
    "os.makedirs(frames_folder, exist_ok=True)\n",
    "\n",
    "# save the frames to disk\n",
    "for i, frame in enumerate(frames):\n",
    "    cv2.imwrite(f\"{frames_folder}/frame_{i:03}.png\", frame)\n",
    "\n",
    "print(f\"Saved {len(frames)} frames to disk at '{frames_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:20<00:00, 195.55it/s]\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\n",
    "    \"train_loss\", [\"+experiment/train_loss=latent_encoder_shapenet_chair_4096\"]\n",
    ")\n",
    "metainfo = MetaInfo(cfg.data.data_dir, split=\"val_latent\")\n",
    "metainfo.load_snn()\n",
    "sampler = ChunkSampler(metainfo.snn_labels, chunk_size=1)\n",
    "max([i for i in sampler]), len([i for i in sampler])\n",
    "loss_ckpt_path = \"/home/borth/sketch2shape/logs/train_latent_encoder/runs/2024-02-15_22-31-15/checkpoints/epoch_019.ckpt\"\n",
    "shape_view_id = 11\n",
    "shape_k = 16\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.loss_ckpt_path = loss_ckpt_path\n",
    "cfg.model.shape_k = shape_k\n",
    "cfg.model.shape_view_id = shape_view_id\n",
    "cfg.model.shape_init = True\n",
    "cfg.model.obj_id = metainfo.obj_ids[0]\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "\n",
    "model.deepsdf.create_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korth/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize((256, 256)),\n",
    "    # v2.CenterCrop(256),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "def load_handrawn(img_path):\n",
    "    # load img, convert to torch tensor and resize to 256x256\n",
    "    img1 = plt.imread(img_path)\n",
    "    img1 = img1 / 255.0\n",
    "\n",
    "    return transform(img1).to(torch.float32)\n",
    "\n",
    "def load_sketch(img_path):\n",
    "    # load img, convert to torch tensor and resize to 256x256\n",
    "    img1 = plt.imread(img_path)\n",
    "    return transform(img1).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korth/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Process each frame and extract normals by rendering the latents\n",
    "visualize = False\n",
    "\n",
    "original_images = []\n",
    "normal_images = []\n",
    "\n",
    "for path in sorted(glob.glob(frames_folder + \"/*.png\")):\n",
    "    if \"handrawn\" in path:\n",
    "        img = load_handrawn(path)\n",
    "    else:\n",
    "        img = load_sketch(path)\n",
    "    with torch.no_grad():\n",
    "        latent = model.loss(img.unsqueeze(0).cuda())\n",
    "        normals = model.deepsdf.capture_camera_frame(latent.squeeze())\n",
    "    \n",
    "    # Save original and normal images into separate lists\n",
    "    original_images.append(plt.imread(path))\n",
    "    normal_images.append(normals.cpu().numpy())\n",
    "    \n",
    "    if visualize:\n",
    "        # Print original image, preprocessed image, and normals\n",
    "        plt.figure()\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original\")\n",
    "        plt.imshow(plt.imread(path))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Preprocessed\")\n",
    "        plt.imshow(img.cpu().numpy().transpose(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Normals\")\n",
    "        plt.imshow(normals.cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos created successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Define the output video filenames\n",
    "original_video_filename = f\"{video_folder}/original_video.mp4\"\n",
    "normal_video_filename = f\"{video_folder}/normal_video.mp4\"\n",
    "\n",
    "# Define the video codec and frame rate\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps = 10\n",
    "\n",
    "res_original = original_images[0].shape[0]\n",
    "\n",
    "# Create the video writers\n",
    "original_video_writer = cv2.VideoWriter(original_video_filename, fourcc, fps, (res_original, res_original))\n",
    "normal_video_writer = cv2.VideoWriter(normal_video_filename, fourcc, fps, (256, 256))\n",
    "\n",
    "# Write the frames to the videos\n",
    "for idx, frame in enumerate(original_images):\n",
    "    original_video_writer.write((frame*255).astype(np.uint8))\n",
    "\n",
    "for frame in normal_images:\n",
    "    normal_video_writer.write((frame*255).astype(np.uint8))\n",
    "\n",
    "# Release the video writers\n",
    "original_video_writer.release()\n",
    "normal_video_writer.release()\n",
    "\n",
    "print(\"Videos created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side by Side Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side by side video created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the output video filename\n",
    "side_by_side_video_filename = f\"{video_folder}/side_by_side_video.mp4\"\n",
    "\n",
    "# Create the video writer\n",
    "side_by_side_video_writer = cv2.VideoWriter(side_by_side_video_filename, fourcc, 5, (392, 196))\n",
    "\n",
    "# Write the frames to the video\n",
    "for idx, (original_frame, normal_frame) in enumerate(zip(original_images, normal_images)):\n",
    "    # Resize the frames to have the same height\n",
    "    original_frame_resized = cv2.resize(original_frame, (196, 196))\n",
    "    normal_frame_resized = cv2.resize(normal_frame, (196, 196))\n",
    "    \n",
    "    # Concatenate the frames side by side\n",
    "    side_by_side_frame = cv2.hconcat([original_frame_resized, normal_frame_resized])\n",
    "    \n",
    "    # Write the side by side frame to the video\n",
    "    side_by_side_video_writer.write((side_by_side_frame*255).astype(np.uint8))\n",
    "\n",
    "# Release the video writer\n",
    "side_by_side_video_writer.release()\n",
    "\n",
    "print(\"Side by side video created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
